"""
core/advanced_asr.py
ASR Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ ØªØµØ­ÛŒØ­ Ø®ÙˆØ¯Ú©Ø§Ø± ÙØ§Ø±Ø³ÛŒ - Ù†Ø³Ø®Ù‡ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡
"""
from faster_whisper import WhisperModel
from typing import Dict, List, Tuple
import re

try:
    from hazm import Normalizer, Lemmatizer
    HAZM_AVAILABLE = True
except ImportError:
    HAZM_AVAILABLE = False
    print("âš ï¸ hazm not available")

# parsivar Ø±Ø§ Ø¨Ø§ Ø§Ø­ØªÛŒØ§Ø· Ù„ÙˆØ¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
PARSIVAR_AVAILABLE = False
parsivar_normalizer = None

try:
    from parsivar import Normalizer as ParsivarNormalizer
    parsivar_normalizer = ParsivarNormalizer()
    PARSIVAR_AVAILABLE = True
except Exception as e:
    print(f"âš ï¸ parsivar Normalizer not available: {e}")

# SpellCheck Ø±Ø§ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ ØªØ³Øª Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… (Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø± Ø§Ø³Øª)
SPELL_CHECK_AVAILABLE = False
spell_checker = None

try:
    from parsivar import SpellCheck
    spell_checker = SpellCheck()
    SPELL_CHECK_AVAILABLE = True
except Exception as e: 
    print(f"âš ï¸ parsivar SpellCheck not available (Ø§ÛŒÙ† Ø¹Ø§Ø¯ÛŒ Ø§Ø³Øª): {e}")


class EnhancedPersianNormalizer:
    """ØªØµØ­ÛŒØ­ Ùˆ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ"""
    
    def __init__(self):
        # hazm
        self.hazm_normalizer = Normalizer() if HAZM_AVAILABLE else None
        self.lemmatizer = Lemmatizer() if HAZM_AVAILABLE else None
        
        # parsivar (ÙÙ‚Ø· normalizerØŒ Ù†Ù‡ spell checker)
        self.parsivar_normalizer = parsivar_normalizer
        self.spell_checker = spell_checker if SPELL_CHECK_AVAILABLE else None
        
        # Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ ØªØµØ­ÛŒØ­Ø§Øª Ø±Ø§ÛŒØ¬ Whisper Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
        self.whisper_corrections = {
            # Ø§Ø´ØªØ¨Ø§Ù‡Ø§Øª Ø±Ø§ÛŒØ¬ Whisper
            "Ù…ÛŒØ´Ù‡": "Ù…ÛŒâ€ŒØ´Ù‡",
            "Ù…ÛŒØ´ÙˆØ¯": "Ù…ÛŒâ€ŒØ´ÙˆØ¯",
            "Ù†Ù…ÛŒØ´Ù‡": "Ù†Ù…ÛŒâ€ŒØ´Ù‡",
            "Ù…ÛŒØ®ÙˆØ§Ù…": "Ù…ÛŒâ€ŒØ®ÙˆØ§Ù…",
            "Ù…ÛŒØ®ÙˆØ§Ù‡Ù…": "Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡Ù…",
            "Ù†Ù…ÛŒØªÙˆÙ†Ù…": "Ù†Ù…ÛŒâ€ŒØªÙˆÙ†Ù…",
            "Ù…ÛŒØªÙˆÙ†Ù…": "Ù…ÛŒâ€ŒØªÙˆÙ†Ù…",
            "Ù…ÛŒÚ©Ù†Ù…": "Ù…ÛŒâ€ŒÚ©Ù†Ù…",
            "Ù†Ù…ÛŒÚ©Ù†Ù…": "Ù†Ù…ÛŒâ€ŒÚ©Ù†Ù…",
            "Ù…ÛŒÚ¯Ù…": "Ù…ÛŒâ€ŒÚ¯Ù…",
            "Ù…ÛŒÚ¯ÙˆÛŒÙ…": "Ù…ÛŒâ€ŒÚ¯ÙˆÛŒÙ…",
            "Ù…ÛŒØ¯ÙˆÙ†Ù…": "Ù…ÛŒâ€ŒØ¯ÙˆÙ†Ù…",
            "Ù†Ù…ÛŒØ¯ÙˆÙ†Ù…": "Ù†Ù…ÛŒâ€ŒØ¯ÙˆÙ†Ù…",
            "Ù…ÛŒØ®ÙˆÙ†Ù…": "Ù…ÛŒâ€ŒØ®ÙˆÙ†Ù…",
            "Ù…ÛŒÙ†ÙˆÛŒØ³Ù…": "Ù…ÛŒâ€ŒÙ†ÙˆÛŒØ³Ù…",
            "Ù…ÛŒØ¨ÛŒÙ†Ù…": "Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ù…",
            "Ù…ÛŒØ±Ù…": "Ù…ÛŒâ€ŒØ±Ù…",
            "Ù…ÛŒØ§Ù…": "Ù…ÛŒâ€ŒØ¢Ù…",
            "Ù…ÛŒØ§ÛŒÙ…": "Ù…ÛŒâ€ŒØ¢ÛŒÙ…",
            "Ù…ÛŒÚ©Ù†ÛŒÙ…": "Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…",
            "Ù…ÛŒÚ©Ù†ÛŒØ¯": "Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯",
            "Ù…ÛŒÚ©Ù†Ù†Ø¯": "Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯",
            "Ø¨Ø²Ø§Ø±": "Ø¨Ø°Ø§Ø±",
            "Ø¨Ø²Ø§Ø±ÛŒØ¯": "Ø¨Ø°Ø§Ø±ÛŒØ¯",
            "Ø§ÛŒÙ†Ø¬ÙˆØ±ÛŒ": "Ø§ÛŒÙ†â€ŒØ¬ÙˆØ±ÛŒ",
            "Ø§ÙˆÙ†Ø¬ÙˆØ±ÛŒ": "Ø§ÙˆÙ†â€ŒØ¬ÙˆØ±ÛŒ",
            "Ú†Ø¬ÙˆØ±ÛŒ": "Ú†Ù‡â€ŒØ¬ÙˆØ±ÛŒ",
            "Ù‡Ù…ÛŒÙ†Ø¬ÙˆØ±ÛŒ": "Ù‡Ù…ÛŒÙ†â€ŒØ¬ÙˆØ±ÛŒ",
            
            # Ø§Ø³Ø§Ù…ÛŒ Ø®Ø§Øµ
            "Ø§ÙˆÚ©Ø±Ø§ÛŒÙ†": "Ø§ÙˆÚ©Ø±Ø§ÛŒÙ†",
            "Ø±ÙˆØ³ÛŒÙ‡": "Ø±ÙˆØ³ÛŒÙ‡",
            "Ø¢Ù…Ø±ÛŒÚ©Ø§": "Ø¢Ù…Ø±ÛŒÚ©Ø§",
            "Ø§Ø³Ø±Ø§ÛŒÛŒÙ„": "Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„",
            "Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„": "Ø§Ø³Ø±Ø§Ø¦ÛŒÙ„",
        }
        
        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ regex Ø¨Ø±Ø§ÛŒ ØªØµØ­ÛŒØ­
        self.regex_patterns = [
            # Ø­Ø±ÙˆÙ Ø¹Ø±Ø¨ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
            (r'ÙŠ', 'ÛŒ'),
            (r'Ùƒ', 'Ú©'),
            (r'Ø©', 'Ù‡'),
            (r'Ø¤', 'Ùˆ'),
            (r'Ø¥', 'Ø§'),
            (r'Ø£', 'Ø§'),
            (r'Ù±', 'Ø§'),
            
            # Ø§Ø¹Ø¯Ø§Ø¯ Ø¹Ø±Ø¨ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
            (r'Ù ', 'Û°'), (r'Ù¡', 'Û±'), (r'Ù¢', 'Û²'), (r'Ù£', 'Û³'),
            (r'Ù¤', 'Û´'), (r'Ù¥', 'Ûµ'), (r'Ù¦', 'Û¶'), (r'Ù§', 'Û·'),
            (r'Ù¨', 'Û¸'), (r'Ù©', 'Û¹'),
            
            # Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´ÙˆÙ†Ø¯Ù‡Ø§
            (r'\bÙ…ÛŒ\s+', 'Ù…ÛŒâ€Œ'),
            (r'\bÙ†Ù…ÛŒ\s+', 'Ù†Ù…ÛŒâ€Œ'),
            (r'\bØ¨Ø±\s+Ù…ÛŒ\s+', 'Ø¨Ø±Ù…ÛŒâ€Œ'),
            (r'\bÙ‡Ù…\s+', 'Ù‡Ù…â€Œ'),
            
            # Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾Ø³ÙˆÙ†Ø¯Ù‡Ø§
            (r'\s+Ù‡Ø§\b', 'â€ŒÙ‡Ø§'),
            (r'\s+Ù‡Ø§ÛŒ\b', 'â€ŒÙ‡Ø§ÛŒ'),
            (r'\s+Ø§ÛŒ\b', 'â€ŒØ§ÛŒ'),
            (r'\s+Ø§Ù…\b', 'â€ŒØ§Ù…'),
            (r'\s+Ø§Øª\b', 'â€ŒØ§Øª'),
            (r'\s+Ø§Ø´\b', 'â€ŒØ§Ø´'),
            (r'\s+ØªØ±\b', 'â€ŒØªØ±'),
            (r'\s+ØªØ±ÛŒÙ†\b', 'â€ŒØªØ±ÛŒÙ†'),
            
            # Ú©Ù„Ù…Ø§Øª ØªØ±Ú©ÛŒØ¨ÛŒ
            (r'\bØ¨ÛŒÙ†\s+Ø§Ù„Ù…Ù„Ù„', 'Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„'),
            (r'\bØ¨ÛŒÙ†\s+Ø§Ù„Ù…Ù„Ù„ÛŒ', 'Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„ÛŒ'),
            (r'\bÙ…Ø§\s+ÙÙˆÙ‚', 'Ù…Ø§ÙÙˆÙ‚'),
            (r'\bØµØ¯\s+Ø¯Ø±\s+ØµØ¯', 'ØµØ¯Ø¯Ø±ØµØ¯'),
            
            # ÙØ¶Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ùˆ Ø¹Ù„Ø§Ø¦Ù…
            (r'\s+([ØŒ. Ø›: ! ØŸ\)\]\}])', r'\1'),
            (r'([\(\[\{])\s+', r'\1'),
            (r'\s{2,}', ' '),
            
            # Ø®Ø· ØªÛŒØ±Ù‡â€ŒÙ‡Ø§
            (r'--+', 'â€”'),
        ]
    
    def normalize(self, text: str) -> str:
        if not text:
            return text
        
        # Ù…Ø±Ø­Ù„Ù‡ 1: Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø§ hazm
        if self.hazm_normalizer:
            try:
                text = self.hazm_normalizer.normalize(text)
            except Exception as e: 
                print(f"âš ï¸ hazm normalization failed: {e}")
        
        # Ù…Ø±Ø­Ù„Ù‡ 2: Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ parsivar (Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯)
        if self.parsivar_normalizer: 
            try:
                text = self.parsivar_normalizer.normalize(text)
            except Exception as e:
                print(f"âš ï¸ parsivar normalization failed:  {e}")
        
        # Ù…Ø±Ø­Ù„Ù‡ 3: ØªØµØ­ÛŒØ­Ø§Øª Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ
        for wrong, correct in self. whisper_corrections. items():
            text = text.replace(wrong, correct)
        
        # Ù…Ø±Ø­Ù„Ù‡ 4: ØªØµØ­ÛŒØ­Ø§Øª regex
        for pattern, replacement in self.regex_patterns:
            try:
                text = re.sub(pattern, replacement, text)
            except Exception: 
                pass
        
        return text. strip()


def transcribe_advanced(
    wav_path: str,
    model_size: str = "large-v3",
    enable_normalization: bool = True,
    language: str = "fa",
    beam_size: int = 5,
    patience: float = 1.0,
    temperature:  Tuple[float, ... ] = (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),
) -> Dict: 
    """
    Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
    """
    
    print(f"ğŸ”„ Loading Whisper model: {model_size}...")
    
    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ GPU/CPU
    try:
        import torch
        if torch.cuda.is_available():
            device = "cuda"
            compute_type = "float16"
            print("   âœ“ Using GPU (CUDA)")
        else:
            device = "cpu"
            compute_type = "int8"
            print("   âœ“ Using CPU")
    except ImportError: 
        device = "cpu"
        compute_type = "int8"
        print("   âœ“ Using CPU (torch not available)")
    
    model = WhisperModel(
        model_size,
        device=device,
        compute_type=compute_type,
        download_root="D:/models/",
        num_workers=4,
    )
    
    # Normalizer
    normalizer = None
    if enable_normalization:
        try:
            normalizer = EnhancedPersianNormalizer()
            print("   âœ“ Persian normalizer ready")
        except Exception as e:
            print(f"   âš ï¸ Normalizer failed: {e}")
    
    # Prompt Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
    initial_prompt = """
    Ø§ÛŒÙ† ÛŒÚ© ÙˆÛŒØ¯ÛŒÙˆÛŒ ÙØ§Ø±Ø³ÛŒ Ø§Ø³Øª. 
    Ù…Ø­ØªÙˆØ§ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø´Ø§Ù…Ù„ Ø§Ø®Ø¨Ø§Ø±ØŒ Ø¢Ù…ÙˆØ²Ø´ØŒ Ø³ÛŒØ§Ø³ØªØŒ ØªØ§Ø±ÛŒØ®ØŒ ÙˆØ±Ø²Ø´ØŒ Ø³Ø±Ú¯Ø±Ù…ÛŒØŒ Ø¢Ø´Ù¾Ø²ÛŒØŒ
    ÙÙ†Ø§ÙˆØ±ÛŒØŒ Ù…Ø°Ù‡Ø¨ØŒ Ø³Ù„Ø§Ù…ØªØŒ Ø¨Ø§Ø²ÛŒØŒ ÛŒØ§ ÙˆÙ„Ø§Ú¯ Ø¨Ø§Ø´Ø¯.
    Ù„Ø·ÙØ§Ù‹ Ù…ØªÙ† Ø±Ø§ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ Ú©Ù†ÛŒØ¯.
    """
    
    print("ğŸ¤ Transcribing...")
    segments, info = model.transcribe(
        wav_path,
        language=language,
        beam_size=beam_size,
        patience=patience,
        temperature=temperature,
        vad_filter=True,
        vad_parameters={
            "threshold": 0.5,
            "min_speech_duration_ms": 250,
            "min_silence_duration_ms": 100,
            "speech_pad_ms": 30,
        },
        initial_prompt=initial_prompt. strip(),
        condition_on_previous_text=True,
        compression_ratio_threshold=2.4,
        log_prob_threshold=-1.0,
        no_speech_threshold=0.6,
        word_timestamps=True,
    )
    
    # Ù¾Ø±Ø¯Ø§Ø²Ø´ segments
    text_parts = []
    processed_segments = []
    word_list = []
    
    for seg in segments:
        text = seg.text. strip()
        
        if normalizer:
            try:
                text = normalizer.normalize(text)
            except Exception: 
                pass
        
        if text:
            text_parts.append(text)
            
            seg_data = {
                "start":  round(seg.start, 2),
                "end": round(seg. end, 2),
                "text":  text,
            }
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ú©Ù„Ù…Ø§Øª Ø¨Ø§ timestamp
            if hasattr(seg, 'words') and seg.words:
                seg_data["words"] = []
                for w in seg.words:
                    word_text = w.word
                    if normalizer:
                        try:
                            word_text = normalizer.normalize(word_text)
                        except: 
                            pass
                    seg_data["words"].append({
                        "word": word_text,
                        "start": round(w.start, 2),
                        "end": round(w.end, 2),
                        "probability": round(w.probability, 3)
                    })
                word_list.extend(seg_data["words"])
            
            processed_segments. append(seg_data)
    
    full_text = " ". join(text_parts).strip()
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ø­ØªÙ…Ø§Ù„ Ú©Ù„Ù…Ø§Øª
    avg_word_prob = 0.0
    if word_list: 
        avg_word_prob = sum(w["probability"] for w in word_list) / len(word_list)
    
    return {
        "language": info.language,
        "language_probability": round(info.language_probability, 4),
        "text":  full_text,
        "segments": processed_segments,
        "word_count": len(full_text.split()),
        "duration":  round(info.duration, 2) if hasattr(info, 'duration') else None,
        "avg_word_confidence": round(avg_word_prob, 3),
        "transcription_quality": _assess_quality(avg_word_prob, info.language_probability),
    }


def _assess_quality(avg_word_prob: float, lang_prob: float) -> str:
    """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ø±ÙˆÙ†ÙˆÛŒØ³ÛŒ"""
    score = (avg_word_prob * 0.7) + (lang_prob * 0.3)
    if score >= 0.85:
        return "excellent"
    elif score >= 0.7:
        return "good"
    elif score >= 0.5:
        return "fair"
    else: 
        return "poor"